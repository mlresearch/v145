---
title: Kernel-Based Smoothness Analysis of Residual Networks
abstract: 'A major factor in the success of deep neural networks is the use of sophisticated
  architectures rather than the classical multilayer perceptron (MLP). Residual networks
  (ResNets) stand out among these powerful modern architectures. Previous works focused
  on the optimization advantages of deep ResNets over deep MLPs. In this paper, we
  show another distinction between the two models, namely, a tendency of ResNets to
  promote smoother interpolations than MLPs. We analyze this phenomenon via the neural
  tangent kernel (NTK) approach. First, we compute the NTK for a considered ResNet
  model and prove its stability during gradient descent training. Then, we show by
  various evaluation methodologies that for ReLU activations the NTK of ResNet, and
  its kernel regression results, are smoother than the ones of MLP. The better smoothness
  observed in our analysis may explain the better generalization ability of ResNets
  and the practice of moderately attenuating the residual blocks. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tirer22a
month: 0
tex_title: Kernel-Based Smoothness Analysis of Residual Networks
firstpage: 921
lastpage: 954
page: 921-954
order: 921
cycles: false
bibtex_author: Tirer, Tom and Bruna, Joan and Giryes, Raja
author:
- given: Tom
  family: Tirer
- given: Joan
  family: Bruna
- given: Raja
  family: Giryes
date: 2022-04-30
address:
container-title: Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference
volume: '145'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 4
  - 30
pdf: https://proceedings.mlr.press/v145/tirer22a/tirer22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
