---
title: On the emergence of simplex symmetry in the final and penultimate layers of
  neural network classifiers
abstract: 'A recent numerical study observed that neural network classifiers enjoy
  a large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
  where $A$ is a linear map and $f$ is the output of the penultimate layer of the
  network (after activation), then all data points $x_{i, 1}, â€¦, x_{i, N_i}$ in a
  class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$ are located
  at the vertices of a regular $k-1$-dimensional \sw{standard simplex} in a high-dimensional
  Euclidean space. We explain this observation analytically in toy models for highly
  expressive deep neural networks. In complementary examples, we demonstrate rigorously
  that even the final output of the classifier $h$ is not uniform over data samples
  from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not bring
  the data samples into a convenient geometric configuration). '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: e22b
month: 0
tex_title: On the emergence of simplex symmetry in the final and penultimate layers
  of neural network classifiers
firstpage: 270
lastpage: 290
page: 270-290
order: 270
cycles: false
bibtex_author: E, Weinan and Wojtowytsch, Stephan
author:
- given: Weinan
  family: E
- given: Stephan
  family: Wojtowytsch
date: 2022-04-30
address:
container-title: Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference
volume: '145'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 4
  - 30
pdf: https://proceedings.mlr.press/v145/e22b/e22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
