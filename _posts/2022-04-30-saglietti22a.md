---
title: Solvable Model for Inheriting the Regularization through Knowledge Distillation
abstract: 'In recent years the empirical success of transfer learning with neural
  networks has stimulated an increasing interest in obtaining a theoretical understanding
  of its core properties. Knowledge Dis- tillation where a smaller neural network
  is trained using the outputs of a larger neural network is a particularly interesting
  case of transfer learning. In the present work, we introduce a statistical physics
  framework that allows an analytic characterization of the properties of knowledge
  distil- lation (KD) in shallow neural networks. Focusing the analysis on a solvable
  model that exhibits a non-trivial generalization gap, we investigate the effectiveness
  of KD. We are able to show that, through KD, the regularization properties of the
  larger teacher model can be inherited by the smaller student and that the yielded
  generalization performance is closely linked to and limited by the op- timality
  of the teacher. Finally, we analyze the double descent phenomenology that can arise
  in the considered KD setting. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saglietti22a
month: 0
tex_title: Solvable Model for Inheriting the Regularization through Knowledge Distillation
firstpage: 809
lastpage: 846
page: 809-846
order: 809
cycles: false
bibtex_author: Saglietti, Luca and Zdeborova, Lenka
author:
- given: Luca
  family: Saglietti
- given: Lenka
  family: Zdeborova
date: 2022-04-30
address:
container-title: Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference
volume: '145'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 4
  - 30
pdf: https://proceedings.mlr.press/v145/saglietti22a/saglietti22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
