---
title: 'Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel Theory?'
abstract: 'Neural Tangent Kernel (NTK) theory is widely used to study the dynamics
  of infinitely-wide deep neural networks (DNNs) under gradient descent. But do the
  results for infinitely-wide networks give us hints about the behavior of real finite-width
  ones? In this paper, we study empirically when NTK theory is valid in practice for
  fully-connected ReLU and sigmoid DNNs. We find out that whether a network is in
  the NTK regime depends on the hyperparameters of random initialization and the network’s
  depth. In particular, NTK theory does not explain the behavior of sufficiently deep
  networks initialized so that their gradients explode as they propagate through the
  network’s layers: the kernel is random at initialization and changes significantly
  during training in this case, contrary to NTK theory. On the other hand, in the
  case of vanishing gradients, DNNs are in the the NTK regime but become untrainable
  rapidly with depth. We also describe a framework to study generalization properties
  of DNNs, in particular the variance of network’s output function, by means of NTK
  theory and discuss its limits.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: seleznova22a
month: 0
tex_title: 'Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel Theory?'
firstpage: 868
lastpage: 895
page: 868-895
order: 868
cycles: false
bibtex_author: Seleznova, Mariia and Kutyniok, Gitta
author:
- given: Mariia
  family: Seleznova
- given: Gitta
  family: Kutyniok
date: 2022-04-30
address:
container-title: Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference
volume: '145'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 4
  - 30
pdf: https://proceedings.mlr.press/v145/seleznova22a/seleznova22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
